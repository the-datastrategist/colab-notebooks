{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# tDS Quickstart Notebook (Standalone)\n",
        "\n",
        "This notebook is a **standalone**, local-Jupyter-friendly quickstart for common Data Strategist workflows on **Google Cloud**:\n",
        "- Auth (ADC / service account)\n",
        "- BigQuery query + (optional) write\n",
        "- (Optional) Cloud Storage smoke actions\n",
        "- Reproducible configuration + safe-by-default write guards\n",
        "\n",
        "**Design goals**\n",
        "- Runs outside Colab (no `google.colab` dependencies)\n",
        "- Safe defaults (no accidental writes)\n",
        "- Clear \u201cwhat/why\u201d for each step\n",
        "\n",
        "_Last updated: 2026-01-10_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Prerequisites\n",
        "\n",
        "### Local environment\n",
        "- Python 3.10+\n",
        "- Jupyter installed\n",
        "- Google Cloud SDK set up (recommended)\n",
        "\n",
        "### Authentication (choose one)\n",
        "- **Application Default Credentials (recommended)**:\n",
        "  - `gcloud auth application-default login`\n",
        "- **Service account JSON**:\n",
        "  - Set `GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json`\n",
        "\n",
        "> If you don't have GCP credentials configured, read cells will fail with an auth error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: install dependencies (uncomment if running in a fresh environment)\n",
        "# !pip install -U google-cloud-bigquery google-cloud-storage pandas pyarrow\n",
        "\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    # ---- Project / location ----\n",
        "    project_id: str = os.environ.get(\"GOOGLE_PROJECT_ID\", \"\")  # set this\n",
        "    location: str = os.environ.get(\"GOOGLE_LOCATION\", \"US\")\n",
        "\n",
        "    # ---- BigQuery (optional write) ----\n",
        "    allow_writes: bool = os.environ.get(\"ALLOW_WRITES\", \"0\") == \"1\"\n",
        "    dataset_id: str = os.environ.get(\"BQ_DATASET_ID\", \"tds_sandbox\")\n",
        "    table_id: str = os.environ.get(\"BQ_TABLE_ID\", \"quickstart_table\")\n",
        "\n",
        "    # ---- GCS (optional) ----\n",
        "    bucket: str = os.environ.get(\"GCS_BUCKET\", \"\")\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Authentication + Clients\n",
        "\n",
        "We use **google-auth default credentials**. In production, you'd typically run this in:\n",
        "- Cloud Run / GKE (Workload Identity)\n",
        "- Airflow/Composer (service account)\n",
        "- CI (short-lived identity)\n",
        "\n",
        "This cell verifies auth and prints the detected project.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.auth import default as google_auth_default\n",
        "from google.cloud import bigquery\n",
        "\n",
        "creds, detected_project = google_auth_default()\n",
        "project = cfg.project_id or detected_project\n",
        "\n",
        "if not project:\n",
        "    raise ValueError(\n",
        "        \"No project detected. Set GOOGLE_PROJECT_ID env var or configure gcloud ADC.\"\n",
        "    )\n",
        "\n",
        "bq = bigquery.Client(project=project, credentials=creds, location=cfg.location)\n",
        "\n",
        "print(\"Authenticated \u2705\")\n",
        "print(\"Project:\", project)\n",
        "print(\"Location:\", cfg.location)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) BigQuery: Read Example (Public Dataset)\n",
        "\n",
        "This is a safe, read-only query against a public dataset.\n",
        "It demonstrates:\n",
        "- query execution\n",
        "- returning a pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "sql = \"\"\"\n",
        "SELECT\n",
        "  name,\n",
        "  SUM(number) AS total\n",
        "FROM `bigquery-public-data.usa_names.usa_1910_2013`\n",
        "WHERE state = 'CA'\n",
        "GROUP BY name\n",
        "ORDER BY total DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "df = bq.query(sql).to_dataframe()\n",
        "df\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) BigQuery: Optional Write (Guarded)\n",
        "\n",
        "Writes are **disabled by default**. To enable, set:\n",
        "- `ALLOW_WRITES=1`\n",
        "\n",
        "This pattern avoids accidental writes during review or CI.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.api_core.exceptions import NotFound\n",
        "\n",
        "dataset_ref = bigquery.Dataset(f\"{project}.{cfg.dataset_id}\")\n",
        "dataset_ref.location = cfg.location\n",
        "\n",
        "table_ref = f\"{project}.{cfg.dataset_id}.{cfg.table_id}\"\n",
        "\n",
        "if not cfg.allow_writes:\n",
        "    print(\"Writes are disabled (ALLOW_WRITES=0). Skipping dataset/table creation.\")\n",
        "else:\n",
        "    # Create dataset if missing\n",
        "    try:\n",
        "        bq.get_dataset(dataset_ref)\n",
        "        print(\"Dataset exists:\", dataset_ref.dataset_id)\n",
        "    except NotFound:\n",
        "        bq.create_dataset(dataset_ref)\n",
        "        print(\"Created dataset:\", dataset_ref.dataset_id)\n",
        "\n",
        "    # Create table (a tiny example) if missing\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"run_ts\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"message\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    ]\n",
        "    try:\n",
        "        bq.get_table(table_ref)\n",
        "        print(\"Table exists:\", table_ref)\n",
        "    except NotFound:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        bq.create_table(table)\n",
        "        print(\"Created table:\", table_ref)\n",
        "\n",
        "    # Insert one row\n",
        "    rows = [{\"run_ts\": datetime.datetime.utcnow().isoformat(), \"message\": \"hello from tDS quickstart\"}]\n",
        "    errors = bq.insert_rows_json(table_ref, rows)\n",
        "    if errors:\n",
        "        raise RuntimeError(errors)\n",
        "    print(\"Inserted 1 row into:\", table_ref)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) GCS: Optional Smoke Actions (List / Upload)\n",
        "\n",
        "This section is optional and permission-dependent.\n",
        "\n",
        "To enable upload/list:\n",
        "- Set `GCS_BUCKET` to a bucket you have access to\n",
        "- Ensure your credentials include Storage permissions\n",
        "\n",
        "Writes are also guarded by `ALLOW_WRITES=1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "if not cfg.bucket:\n",
        "    print(\"GCS_BUCKET not set. Skipping GCS section.\")\n",
        "else:\n",
        "    gcs = storage.Client(project=project, credentials=creds)\n",
        "    bucket = gcs.bucket(cfg.bucket)\n",
        "\n",
        "    # List first 20 blobs (read-only)\n",
        "    print(f\"Listing blobs in gs://{cfg.bucket} (first 20):\")\n",
        "    for i, blob in enumerate(gcs.list_blobs(cfg.bucket, max_results=20), start=1):\n",
        "        print(f\"{i:02d}. {blob.name}\")\n",
        "\n",
        "    # Optional upload (guarded)\n",
        "    if not cfg.allow_writes:\n",
        "        print(\"Writes are disabled (ALLOW_WRITES=0). Skipping upload.\")\n",
        "    else:\n",
        "        tmp_path = \"tds_quickstart_example.txt\"\n",
        "        with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"hello from tDS quickstart\\n\")\n",
        "\n",
        "        blob = bucket.blob(\"quickstart/tds_quickstart_example.txt\")\n",
        "        blob.upload_from_filename(tmp_path)\n",
        "        print(\"Uploaded:\", f\"gs://{cfg.bucket}/{blob.name}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Production Notes\n",
        "\n",
        "If this were running as a production job, you'd typically add:\n",
        "\n",
        "- **Orchestration**\n",
        "  - Airflow/Composer or Prefect schedules\n",
        "  - Backfills + retries + idempotency\n",
        "\n",
        "- **Observability**\n",
        "  - Structured logs\n",
        "  - Metrics (rows processed, runtime, error rate)\n",
        "  - Data quality checks (nulls, duplicates, range checks)\n",
        "\n",
        "- **Artifact/version management**\n",
        "  - Timestamped outputs\n",
        "  - Metadata tables (run_id, inputs, outputs, git SHA)\n",
        "\n",
        "- **Security**\n",
        "  - Workload Identity (avoid long-lived keys)\n",
        "  - Least-privilege IAM\n"
      ]
    }
  ]
}