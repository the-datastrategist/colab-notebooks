{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# eCommerce Visit Segmentation (Session Clustering) \u2014 Scalable, Production-Minded Baseline\n",
        "\n",
        "This notebook is a **Mercor-style** rewrite of `tds_session_clustering.ipynb` with a focus on:\n",
        "\n",
        "- Correct **visit/session grain** (with guardrails)\n",
        "- Warehouse-first feature engineering (**BigQuery SQL \u2192 visit-level feature table**)\n",
        "- Standardized preprocessing (NA handling, outlier clipping, scaling)\n",
        "- Defensible clustering workflow (K diagnostics + interpretability)\n",
        "- Artifact outputs designed for production (labels, centroids, diagnostics, run report)\n",
        "- **SMOKE_MODE** so the notebook runs end-to-end without BigQuery credentials\n",
        "\n",
        "> You can productionize this by extracting the SQL into `sql/features.sql`, packaging the Python into `src/`, and orchestrating runs with Airflow/Prefect/Dagster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Configuration\n",
        "\n",
        "Environment variables (optional):\n",
        "- `GOOGLE_PROJECT_ID` (required unless `SMOKE_MODE=1`)\n",
        "- `DATASET` (default: `bigquery-public-data.google_analytics_sample`)\n",
        "- `TABLE_PATTERN` (default: `ga_sessions_*`)\n",
        "- `DATE_FROM`, `DATE_TO` (YYYYMMDD; end is exclusive)\n",
        "- `K_MIN`, `K_MAX`, `K_FINAL`\n",
        "- `Z_CLIP` (winsorization threshold in z-score space)\n",
        "- `OUT_DIR` (artifact path)\n",
        "- `SEED`\n",
        "- `SMOKE_MODE=1` (run on synthetic sessions; good for CI / quick review)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    # BigQuery inputs\n",
        "    project_id: str = os.environ.get(\"GOOGLE_PROJECT_ID\", \"\")\n",
        "    dataset: str = os.environ.get(\"DATASET\", \"bigquery-public-data.google_analytics_sample\")\n",
        "    table_pattern: str = os.environ.get(\"TABLE_PATTERN\", \"ga_sessions_*\")\n",
        "    date_from: str = os.environ.get(\"DATE_FROM\", \"20170101\")\n",
        "    date_to: str = os.environ.get(\"DATE_TO\", \"20170201\")  # exclusive end\n",
        "\n",
        "    # Modeling\n",
        "    k_min: int = int(os.environ.get(\"K_MIN\", \"2\"))\n",
        "    k_max: int = int(os.environ.get(\"K_MAX\", \"12\"))\n",
        "    k_final: int = int(os.environ.get(\"K_FINAL\", \"6\"))\n",
        "    z_clip: float = float(os.environ.get(\"Z_CLIP\", \"5.0\"))\n",
        "    seed: int = int(os.environ.get(\"SEED\", \"42\"))\n",
        "\n",
        "    # Outputs\n",
        "    out_dir: str = os.environ.get(\"OUT_DIR\", \"./artifacts\")\n",
        "\n",
        "    # Execution mode\n",
        "    smoke_mode: bool = os.environ.get(\"SMOKE_MODE\", \"0\") == \"1\"\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "np.random.seed(cfg.seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Helper functions (guardrails + preprocessing)\n",
        "\n",
        "Production mindset:\n",
        "- Fail fast on grain violations (`visit_id` must be unique).\n",
        "- Clip extreme outliers before scaling so K-Means doesn't chase anomalies.\n",
        "- Keep the feature set explicit and auditable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def assert_unique_grain(df: pd.DataFrame, key: str = \"visit_id\") -> None:\n",
        "    if key not in df.columns:\n",
        "        raise ValueError(f\"Missing required grain key column: {key}\")\n",
        "    dupes = int(df[key].duplicated().sum())\n",
        "    if dupes:\n",
        "        raise ValueError(\n",
        "            f\"{key} is not unique ({dupes} duplicates). \"\n",
        "            \"Your joins/aggregations likely multiplied rows.\"\n",
        "        )\n",
        "\n",
        "def preprocess_features(df: pd.DataFrame, features: list[str], z_clip: float = 5.0):\n",
        "    missing = [c for c in features if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing feature columns: {missing}\")\n",
        "\n",
        "    X = df[features].copy().fillna(0)\n",
        "\n",
        "    # winsorize via z-score clipping (robust enough baseline; swap with quantile clip if preferred)\n",
        "    mu = X.mean()\n",
        "    sigma = X.std(ddof=0).replace(0, np.nan)\n",
        "    z = (X - mu) / sigma\n",
        "    X_clip = (\n",
        "        X.mask(z > z_clip, mu + z_clip * sigma)\n",
        "         .mask(z < -z_clip, mu - z_clip * sigma)\n",
        "         .fillna(X)\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_clip)\n",
        "\n",
        "    return X_clip, X_scaled, scaler\n",
        "\n",
        "def k_diagnostics(X_scaled: np.ndarray, k_min: int, k_max: int, seed: int) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
        "        labels = km.fit_predict(X_scaled)\n",
        "        rows.append({\n",
        "            \"k\": k,\n",
        "            \"inertia_ssd\": float(km.inertia_),\n",
        "            \"silhouette\": float(silhouette_score(X_scaled, labels)),\n",
        "            \"davies_bouldin\": float(davies_bouldin_score(X_scaled, labels)),\n",
        "            \"calinski_harabasz\": float(calinski_harabasz_score(X_scaled, labels)),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Ingestion\n",
        "\n",
        "Two modes:\n",
        "\n",
        "### A) BigQuery mode (default)\n",
        "- Runs a **visit-level** feature SQL query.\n",
        "- Requires `GOOGLE_PROJECT_ID`.\n",
        "\n",
        "### B) Smoke mode\n",
        "- Set `SMOKE_MODE=1` to run end-to-end on synthetic sessions.\n",
        "- This is useful for CI, quick validation, and reviewer environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def make_synthetic_sessions(n: int = 2000, seed: int = 42) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Three archetypes: bouncers, researchers, buyers\n",
        "    archetype = rng.choice([0, 1, 2], size=n, p=[0.45, 0.45, 0.10])\n",
        "\n",
        "    pageviews = np.where(archetype == 0, rng.poisson(2, n),\n",
        "                np.where(archetype == 1, rng.poisson(8, n), rng.poisson(12, n)))\n",
        "    hits = pageviews + rng.poisson(5, n)\n",
        "    unique_pages = np.minimum(pageviews, np.where(archetype == 0, rng.poisson(2, n), rng.poisson(6, n)))\n",
        "\n",
        "    time_on_site_sec = np.where(archetype == 0, rng.normal(35, 20, n),\n",
        "                       np.where(archetype == 1, rng.normal(240, 80, n), rng.normal(420, 120, n)))\n",
        "    time_on_site_sec = np.clip(time_on_site_sec, 0, None)\n",
        "\n",
        "    transactions = np.where(archetype == 2, rng.binomial(1, 0.55, n), rng.binomial(1, 0.02, n))\n",
        "    transaction_revenue = transactions * np.where(archetype == 2, rng.normal(85000, 25000, n), rng.normal(15000, 5000, n))\n",
        "    transaction_revenue = np.clip(transaction_revenue, 0, None)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"visit_id\": [f\"v{i}\" for i in range(n)],\n",
        "        \"date_id\": pd.to_datetime(\"2026-01-01\"),\n",
        "        \"pageviews\": pageviews.astype(int),\n",
        "        \"hits\": hits.astype(int),\n",
        "        \"unique_pages\": unique_pages.astype(int),\n",
        "        \"time_on_site_sec\": time_on_site_sec.astype(float),\n",
        "        \"transactions\": transactions.astype(int),\n",
        "        \"transaction_revenue\": transaction_revenue.astype(float),\n",
        "    })\n",
        "\n",
        "    df[\"time_on_site_sec_per_pageview\"] = df[\"time_on_site_sec\"] / df[\"pageviews\"].replace(0, 1)\n",
        "    df[\"hits_per_pageview\"] = df[\"hits\"] / df[\"pageviews\"].replace(0, 1)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_feature_query(dataset: str, table_pattern: str, date_from: str, date_to: str) -> str:\n",
        "    # Keep this query small and auditable; extract to sql/features.sql in production.\n",
        "    return f\"\"\"\n",
        "    WITH base AS (\n",
        "      SELECT\n",
        "        CONCAT(CAST(fullVisitorId AS STRING), '-', CAST(visitId AS STRING)) AS visit_id,\n",
        "        PARSE_DATE('%Y%m%d', date) AS date_id,\n",
        "        totals.hits AS hits,\n",
        "        totals.pageviews AS pageviews,\n",
        "        totals.timeOnSite AS time_on_site_sec,\n",
        "        totals.transactions AS transactions,\n",
        "        totals.transactionRevenue AS transaction_revenue\n",
        "      FROM `{dataset}.{table_pattern}`\n",
        "      WHERE _TABLE_SUFFIX >= '{date_from}' AND _TABLE_SUFFIX < '{date_to}'\n",
        "    ),\n",
        "    pages AS (\n",
        "      SELECT\n",
        "        CONCAT(CAST(fullVisitorId AS STRING), '-', CAST(visitId AS STRING)) AS visit_id,\n",
        "        COUNT(DISTINCT h.page.pagePath) AS unique_pages\n",
        "      FROM `{dataset}.{table_pattern}`, UNNEST(hits) h\n",
        "      WHERE _TABLE_SUFFIX >= '{date_from}' AND _TABLE_SUFFIX < '{date_to}'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "    SELECT\n",
        "      b.*,\n",
        "      p.unique_pages,\n",
        "      SAFE_DIVIDE(b.time_on_site_sec, NULLIF(b.pageviews, 0)) AS time_on_site_sec_per_pageview,\n",
        "      SAFE_DIVIDE(b.hits, NULLIF(b.pageviews, 0)) AS hits_per_pageview\n",
        "    FROM base b\n",
        "    LEFT JOIN pages p USING (visit_id)\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_from_bigquery(cfg: Config) -> pd.DataFrame:\n",
        "    if not cfg.project_id:\n",
        "        raise ValueError(\"GOOGLE_PROJECT_ID is required unless SMOKE_MODE=1.\")\n",
        "    from google.cloud import bigquery\n",
        "    client = bigquery.Client(project=cfg.project_id)\n",
        "    sql = build_feature_query(cfg.dataset, cfg.table_pattern, cfg.date_from, cfg.date_to)\n",
        "    return client.query(sql).result().to_dataframe(create_bqstorage_client=True)\n",
        "\n",
        "df = make_synthetic_sessions(seed=cfg.seed) if cfg.smoke_mode else load_from_bigquery(cfg)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Guardrails (grain + basic validity)\n",
        "\n",
        "These checks prevent silent metric corruption (common failure mode in analytics pipelines).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "assert_unique_grain(df, \"visit_id\")\n",
        "\n",
        "for col in [\"pageviews\", \"hits\", \"time_on_site_sec\"]:\n",
        "    if (df[col].fillna(0) < 0).any():\n",
        "        raise ValueError(f\"Invalid negative values detected in {col}.\")\n",
        "\n",
        "df.agg(\n",
        "    n_rows=(\"visit_id\", \"size\"),\n",
        "    n_visits=(\"visit_id\", \"nunique\"),\n",
        "    min_date=(\"date_id\", \"min\"),\n",
        "    max_date=(\"date_id\", \"max\"),\n",
        ").to_frame(\"value\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Feature set + preprocessing\n",
        "\n",
        "Keep feature selection **small and auditable**.\n",
        "In production you would:\n",
        "- own a feature contract (schema + tests)\n",
        "- evaluate drift on each feature\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "FEATURES = [\n",
        "    \"pageviews\",\n",
        "    \"hits\",\n",
        "    \"unique_pages\",\n",
        "    \"time_on_site_sec\",\n",
        "    \"time_on_site_sec_per_pageview\",\n",
        "    \"hits_per_pageview\",\n",
        "    \"transactions\",\n",
        "    \"transaction_revenue\",\n",
        "]\n",
        "\n",
        "X_clip, X_scaled, scaler = preprocess_features(df, FEATURES, z_clip=cfg.z_clip)\n",
        "X_clip.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Choose K (multi-metric diagnostics)\n",
        "\n",
        "We use several diagnostics together. Final K should also pass:\n",
        "- interpretability\n",
        "- cluster size sanity (no tiny artifact clusters unless expected)\n",
        "- temporal stability (segment mix doesn't thrash)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "diag = k_diagnostics(X_scaled, cfg.k_min, cfg.k_max, cfg.seed)\n",
        "diag\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(diag[\"k\"], diag[\"inertia_ssd\"], marker=\"o\")\n",
        "plt.title(\"SSD / Inertia (Elbow)\")\n",
        "plt.xlabel(\"k\"); plt.ylabel(\"inertia\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(diag[\"k\"], diag[\"silhouette\"], marker=\"o\")\n",
        "plt.title(\"Silhouette (higher is better)\")\n",
        "plt.xlabel(\"k\"); plt.ylabel(\"score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(diag[\"k\"], diag[\"davies_bouldin\"], marker=\"o\")\n",
        "plt.title(\"Davies\u2013Bouldin (lower is better)\")\n",
        "plt.xlabel(\"k\"); plt.ylabel(\"score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Fit final model + interpret clusters\n",
        "\n",
        "Deliverables:\n",
        "- `session_clusters`: visit_id \u2192 cluster label\n",
        "- `cluster_centers`: centroids in original feature units\n",
        "- size distribution for sanity checks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = KMeans(n_clusters=cfg.k_final, n_init=\"auto\", random_state=cfg.seed)\n",
        "labels = model.fit_predict(X_scaled)\n",
        "\n",
        "session_clusters = df[[\"visit_id\", \"date_id\"]].copy()\n",
        "session_clusters[\"cluster\"] = labels.astype(int)\n",
        "\n",
        "sizes = session_clusters[\"cluster\"].value_counts().sort_index().to_frame(\"n_sessions\")\n",
        "sizes[\"pct_sessions\"] = sizes[\"n_sessions\"] / sizes[\"n_sessions\"].sum()\n",
        "sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cluster_centers = pd.DataFrame(\n",
        "    scaler.inverse_transform(model.cluster_centers_),\n",
        "    columns=FEATURES,\n",
        ")\n",
        "cluster_centers[\"cluster\"] = range(cfg.k_final)\n",
        "\n",
        "# Helpful for interpretation: order clusters by \"intent proxy\" (transactions/revenue)\n",
        "cluster_centers.sort_values([\"transactions\", \"transaction_revenue\"], ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Temporal stability (segment mix drift)\n",
        "\n",
        "In production, drift monitoring is often more valuable than a one-time clustering metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "mix = session_clusters.groupby([\"date_id\", \"cluster\"]).size().rename(\"n\").reset_index()\n",
        "tot = mix.groupby(\"date_id\")[\"n\"].sum().rename(\"n_total\").reset_index()\n",
        "mix = mix.merge(tot, on=\"date_id\", how=\"left\")\n",
        "mix[\"pct\"] = mix[\"n\"] / mix[\"n_total\"]\n",
        "\n",
        "pivot = mix.pivot_table(index=\"date_id\", columns=\"cluster\", values=\"pct\", fill_value=0).sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for c in pivot.columns:\n",
        "    plt.plot(pivot.index, pivot[c], label=f\"cluster {c}\")\n",
        "plt.title(\"Cluster Mix Over Time (pct sessions)\")\n",
        "plt.xlabel(\"date\"); plt.ylabel(\"pct\")\n",
        "plt.legend(ncol=3, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Persist artifacts\n",
        "\n",
        "Artifacts are the minimum useful outputs for production use:\n",
        "- session labels\n",
        "- centroids\n",
        "- k diagnostics\n",
        "- run report (config + stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "Path(cfg.out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "session_clusters_path = Path(cfg.out_dir) / \"session_clusters.parquet\"\n",
        "cluster_centers_path = Path(cfg.out_dir) / \"cluster_centers.parquet\"\n",
        "k_diag_path = Path(cfg.out_dir) / \"k_diagnostics.parquet\"\n",
        "report_path = Path(cfg.out_dir) / \"model_report.json\"\n",
        "\n",
        "session_clusters.to_parquet(session_clusters_path, index=False)\n",
        "cluster_centers.to_parquet(cluster_centers_path, index=False)\n",
        "diag.to_parquet(k_diag_path, index=False)\n",
        "\n",
        "report = {\n",
        "    \"run_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"smoke_mode\": bool(cfg.smoke_mode),\n",
        "    \"config\": cfg.__dict__,\n",
        "    \"n_sessions\": int(len(session_clusters)),\n",
        "    \"k_final\": int(cfg.k_final),\n",
        "    \"cluster_sizes\": sizes.reset_index().rename(columns={\"index\": \"cluster\"}).to_dict(orient=\"records\"),\n",
        "}\n",
        "report_path.write_text(json.dumps(report, indent=2))\n",
        "\n",
        "[str(session_clusters_path), str(cluster_centers_path), str(k_diag_path), str(report_path)]\n"
      ]
    }
  ]
}