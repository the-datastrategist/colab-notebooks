{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Memory-Based Collaborative Filtering (Production-Minded Baseline)\n",
        "\n",
        "This notebook builds an **item\u2013item collaborative filtering** baseline from a MovieLens-style `ratings.csv`.\n",
        "\n",
        "**What this notebook demonstrates (for a production-minded review):**\n",
        "- A clear **data model** (events \u2192 sparse matrix)\n",
        "- A scalable approach: **Top-K neighbors** via cosine similarity (**no dense N\u00d7N similarity matrices**)\n",
        "- A minimal **offline evaluation** (time-aware leave-last-out)\n",
        "- Reproducibility: configuration, seeding, and artifact outputs\n",
        "\n",
        "> \u26a0\ufe0f Compute note: similarity over all items/users can be expensive. This notebook defaults to sampling to keep runtime reasonable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Configuration (edit here) ---\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Config:\n",
        "    # Data: local path or gs://bucket/path/ratings.csv\n",
        "    data_path: str = os.environ.get(\"DATA_PATH\", \"gs://butterstick2023/ml-25m/ratings.csv\")\n",
        "\n",
        "    # Sampling to keep the notebook fast (set to None to disable)\n",
        "    max_users: int | None = int(os.environ.get(\"MAX_USERS\", \"5000\"))\n",
        "    max_items: int | None = int(os.environ.get(\"MAX_ITEMS\", \"20000\"))\n",
        "\n",
        "    # Similarity / retrieval\n",
        "    top_k: int = int(os.environ.get(\"TOP_K\", \"50\"))\n",
        "\n",
        "    # Outputs\n",
        "    out_dir: str = os.environ.get(\"OUT_DIR\", \"./artifacts\")\n",
        "\n",
        "    # Randomness / reproducibility\n",
        "    seed: int = int(os.environ.get(\"SEED\", \"42\"))\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Imports ---\n",
        "import json\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "from sklearn.neighbors import NearestNeighbors\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Import\n",
        "\n",
        "Expected schema (MovieLens-style):\n",
        "\n",
        "- `userId` (int)\n",
        "- `movieId` (int)\n",
        "- `rating` (float)\n",
        "- `timestamp` (int, unix seconds)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _read_csv_any(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Read a CSV from local disk or GCS (gs://...).\n",
        "\n",
        "    Notes:\n",
        "    - For `gs://...`, you need either:\n",
        "      - `gcsfs` installed (pandas can read gs:// directly), OR\n",
        "      - `google-cloud-storage` to download the object first.\n",
        "    \"\"\"\n",
        "    if path.startswith(\"gs://\"):\n",
        "        try:\n",
        "            # If gcsfs is available, this just works.\n",
        "            return pd.read_csv(path)\n",
        "        except Exception as e:\n",
        "            # Fallback: download via GCS client.\n",
        "            from google.cloud import storage  # optional dependency\n",
        "            import tempfile\n",
        "\n",
        "            bucket_name, blob_path = path[5:].split(\"/\", 1)\n",
        "            client = storage.Client()\n",
        "            bucket = client.bucket(bucket_name)\n",
        "            blob = bucket.blob(blob_path)\n",
        "\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False) as tmp:\n",
        "                blob.download_to_filename(tmp.name)\n",
        "                return pd.read_csv(tmp.name)\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "\n",
        "def load_ratings(path: str) -> pd.DataFrame:\n",
        "    df = _read_csv_any(path)\n",
        "\n",
        "    required = {\"userId\", \"movieId\", \"rating\", \"timestamp\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
        "\n",
        "    df = df[list(required)].copy()\n",
        "    df[\"userId\"] = df[\"userId\"].astype(\"int64\")\n",
        "    df[\"movieId\"] = df[\"movieId\"].astype(\"int64\")\n",
        "    df[\"rating\"] = df[\"rating\"].astype(\"float32\")\n",
        "    df[\"timestamp\"] = df[\"timestamp\"].astype(\"int64\")\n",
        "    df[\"event_ts\"] = pd.to_datetime(df[\"timestamp\"], unit=\"s\", utc=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df = load_ratings(cfg.data_path)\n",
        "df.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Basic sanity checks\n",
        "df.agg(\n",
        "    n_rows=(\"rating\", \"size\"),\n",
        "    n_users=(\"userId\", \"nunique\"),\n",
        "    n_items=(\"movieId\", \"nunique\"),\n",
        "    min_ts=(\"event_ts\", \"min\"),\n",
        "    max_ts=(\"event_ts\", \"max\"),\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling (to keep runtime reasonable)\n",
        "\n",
        "This notebook defaults to a user/item cap for review-time execution.\n",
        "For production, you would run this as a batch job (Spark/BigQuery/Ray) and write versioned artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sample_interactions(\n",
        "    df: pd.DataFrame,\n",
        "    max_users: int | None,\n",
        "    max_items: int | None,\n",
        "    seed: int,\n",
        ") -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    if max_users is not None and df[\"userId\"].nunique() > max_users:\n",
        "        users = rng.choice(df[\"userId\"].unique(), size=max_users, replace=False)\n",
        "        df = df[df[\"userId\"].isin(users)]\n",
        "\n",
        "    if max_items is not None and df[\"movieId\"].nunique() > max_items:\n",
        "        items = rng.choice(df[\"movieId\"].unique(), size=max_items, replace=False)\n",
        "        df = df[df[\"movieId\"].isin(items)]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_s = sample_interactions(df, cfg.max_users, cfg.max_items, cfg.seed).copy()\n",
        "df_s.agg(n_rows=(\"rating\",\"size\"), n_users=(\"userId\",\"nunique\"), n_items=(\"movieId\",\"nunique\"))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Two simple, explainable features:\n",
        "- `avg_rating_user`: mean rating per user\n",
        "- `liked`: rating > user mean (implicit feedback proxy)\n",
        "- `wt_rating`: rating normalized by user mean\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reproducibility\n",
        "random.seed(cfg.seed)\n",
        "np.random.seed(cfg.seed)\n",
        "\n",
        "user_mean = df_s.groupby(\"userId\")[\"rating\"].mean().rename(\"avg_rating_user\")\n",
        "df_s = df_s.join(user_mean, on=\"userId\")\n",
        "df_s[\"liked\"] = df_s[\"rating\"] > df_s[\"avg_rating_user\"]\n",
        "df_s[\"wt_rating\"] = (df_s[\"rating\"] / df_s[\"avg_rating_user\"]).astype(\"float32\")\n",
        "\n",
        "df_s[[\"userId\",\"movieId\",\"rating\",\"avg_rating_user\",\"liked\",\"wt_rating\"]].head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Test Split (time-aware)\n",
        "\n",
        "For each user, hold out their last interaction as a simple offline eval target.\n",
        "This prevents leakage from \u201cfuture\u201d interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def leave_last_out(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df = df.sort_values([\"userId\", \"event_ts\"])\n",
        "    idx_last = df.groupby(\"userId\").tail(1).index\n",
        "    test = df.loc[idx_last].copy()\n",
        "    train = df.drop(idx_last).copy()\n",
        "    return train, test\n",
        "\n",
        "train_df, test_df = leave_last_out(df_s)\n",
        "\n",
        "train_df.shape, test_df.shape\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Sparse User\u2013Item Matrix (CSR)\n",
        "\n",
        "Why CSR:\n",
        "- Dense pivots explode memory for large U\u00d7I\n",
        "- CSR supports fast linear algebra and nearest-neighbor retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_user_item_csr(\n",
        "    df: pd.DataFrame,\n",
        "    user_col: str = \"userId\",\n",
        "    item_col: str = \"movieId\",\n",
        "    value_col: str = \"wt_rating\",\n",
        ") -> Tuple[sparse.csr_matrix, np.ndarray, np.ndarray]:\n",
        "    # Map ids \u2192 contiguous indices\n",
        "    user_ids, user_idx = np.unique(df[user_col].to_numpy(), return_inverse=True)\n",
        "    item_ids, item_idx = np.unique(df[item_col].to_numpy(), return_inverse=True)\n",
        "\n",
        "    data = df[value_col].to_numpy(dtype=np.float32)\n",
        "    mat = sparse.csr_matrix((data, (user_idx, item_idx)), shape=(len(user_ids), len(item_ids)))\n",
        "    return mat, user_ids, item_ids\n",
        "\n",
        "X_train, user_ids, item_ids = build_user_item_csr(train_df)\n",
        "X_train.shape, X_train.nnz\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Item\u2013Item Top-K Similarity (production-friendly)\n",
        "\n",
        "Instead of computing a dense item\u00d7item similarity matrix, we retrieve **Top-K neighbors** per item.\n",
        "\n",
        "Why:\n",
        "- Dense similarity is O(I\u00b2) memory\n",
        "- For serving, you only need top neighbors for candidate generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit nearest-neighbors on item vectors (columns) -> use X_train.T (items x users)\n",
        "nn = NearestNeighbors(\n",
        "    n_neighbors=cfg.top_k + 1,  # +1 includes the item itself at distance 0\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "nn.fit(X_train.T)\n",
        "\n",
        "distances, indices = nn.kneighbors(X_train.T, return_distance=True)\n",
        "# Convert cosine distance to cosine similarity\n",
        "similarities = 1.0 - distances\n",
        "\n",
        "# Drop self-match at position 0\n",
        "neighbor_indices = indices[:, 1:]\n",
        "neighbor_sims = similarities[:, 1:]\n",
        "\n",
        "neighbor_indices.shape, neighbor_sims.shape\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recommendation Functions\n",
        "\n",
        "Two practical entry points:\n",
        "1. `similar_items(movie_id)` \u2014 item-to-item lookup (great for explainability)\n",
        "2. `recommend_for_user(user_id)` \u2014 aggregate neighbors of items the user liked\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build fast lookup: item_id -> row index\n",
        "item_id_to_row = {int(mid): i for i, mid in enumerate(item_ids)}\n",
        "\n",
        "def similar_items(movie_id: int, k: int = 10) -> pd.DataFrame:\n",
        "    if movie_id not in item_id_to_row:\n",
        "        raise KeyError(f\"movieId={movie_id} not in training set (sampled).\")\n",
        "\n",
        "    row = item_id_to_row[movie_id]\n",
        "    nbr_rows = neighbor_indices[row, :k]\n",
        "    sims = neighbor_sims[row, :k]\n",
        "    return pd.DataFrame({\n",
        "        \"movieId\": item_ids[nbr_rows].astype(int),\n",
        "        \"similarity\": sims.astype(float),\n",
        "    }).sort_values(\"similarity\", ascending=False)\n",
        "\n",
        "similar_items(movie_id=int(item_ids[0]), k=10).head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Precompute user->row mapping for the sampled training set\n",
        "user_id_to_row = {int(uid): i for i, uid in enumerate(user_ids)}\n",
        "\n",
        "def recommend_for_user(user_id: int, k: int = 10, liked_only: bool = True) -> pd.DataFrame:\n",
        "    if user_id not in user_id_to_row:\n",
        "        raise KeyError(f\"userId={user_id} not in training set (sampled).\")\n",
        "\n",
        "    urow = user_id_to_row[user_id]\n",
        "    user_vec = X_train[urow]\n",
        "\n",
        "    # Items the user interacted with (non-zero entries)\n",
        "    seen_item_rows = set(user_vec.indices.tolist())\n",
        "\n",
        "    # Optionally restrict to implicit positives (liked); otherwise use all interactions.\n",
        "    if liked_only:\n",
        "        # Identify liked items for this user from the training dataframe\n",
        "        liked_items = train_df[(train_df[\"userId\"] == user_id) & (train_df[\"liked\"])].movieId.unique()\n",
        "        liked_rows = [item_id_to_row[int(mid)] for mid in liked_items if int(mid) in item_id_to_row]\n",
        "        seed_rows = liked_rows if liked_rows else list(seen_item_rows)\n",
        "    else:\n",
        "        seed_rows = list(seen_item_rows)\n",
        "\n",
        "    # Aggregate neighbor scores\n",
        "    scores = {}\n",
        "    for r in seed_rows:\n",
        "        for nbr_r, sim in zip(neighbor_indices[r], neighbor_sims[r]):\n",
        "            if nbr_r in seen_item_rows:  # don't recommend already-seen items\n",
        "                continue\n",
        "            scores[nbr_r] = max(scores.get(nbr_r, 0.0), float(sim))  # max-sim aggregator (simple baseline)\n",
        "\n",
        "    top = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    return pd.DataFrame({\n",
        "        \"movieId\": [int(item_ids[r]) for r, _ in top],\n",
        "        \"score\": [s for _, s in top],\n",
        "    })\n",
        "\n",
        "# Example\n",
        "example_user = int(user_ids[0])\n",
        "recommend_for_user(example_user, k=10).head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Offline Evaluation (HitRate@K)\n",
        "\n",
        "We evaluate whether the held-out last item for each user appears in the Top-K recommendations.\n",
        "\n",
        "This is intentionally lightweight but demonstrates:\n",
        "- leakage-aware splitting\n",
        "- a concrete metric tied to ranking quality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hitrate_at_k(k: int = 10) -> float:\n",
        "    hits = 0\n",
        "    total = 0\n",
        "    for uid, grp in test_df.groupby(\"userId\"):\n",
        "        uid = int(uid)\n",
        "        # If user not in training after sampling/split, skip.\n",
        "        if uid not in user_id_to_row:\n",
        "            continue\n",
        "        held_out = int(grp.iloc[0][\"movieId\"])\n",
        "        # If held-out item not in training item universe, skip (sampled out)\n",
        "        if held_out not in item_id_to_row:\n",
        "            continue\n",
        "\n",
        "        recs = recommend_for_user(uid, k=k)\n",
        "        if held_out in set(recs[\"movieId\"].tolist()):\n",
        "            hits += 1\n",
        "        total += 1\n",
        "    return hits / total if total else float(\"nan\")\n",
        "\n",
        "for k in [5, 10, 20, 50]:\n",
        "    print(f\"HitRate@{k}: {hitrate_at_k(k):.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persist Artifacts\n",
        "\n",
        "Artifacts are written locally to `cfg.out_dir`:\n",
        "- `user_item_matrix_train.npz` (CSR sparse)\n",
        "- `item_neighbors.parquet` (top-k neighbors per item)\n",
        "\n",
        "Optional: upload these to object storage (GCS/S3) in a versioned path.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "# 1) Sparse matrix\n",
        "sparse.save_npz(os.path.join(cfg.out_dir, \"user_item_matrix_train.npz\"), X_train)\n",
        "\n",
        "# 2) Neighbors table\n",
        "rows = []\n",
        "for i, mid in enumerate(item_ids):\n",
        "    for nbr_r, sim in zip(neighbor_indices[i], neighbor_sims[i]):\n",
        "        rows.append((int(mid), int(item_ids[nbr_r]), float(sim)))\n",
        "\n",
        "neighbors_df = pd.DataFrame(rows, columns=[\"movieId\", \"neighborMovieId\", \"similarity\"])\n",
        "neighbors_path = os.path.join(cfg.out_dir, \"item_neighbors.parquet\")\n",
        "neighbors_df.to_parquet(neighbors_path, index=False)\n",
        "\n",
        "neighbors_df.head(), neighbors_path\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}